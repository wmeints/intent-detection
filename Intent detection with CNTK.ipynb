{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load the data\n",
    "The first step in the process of making an intent detection model is to load up the data.\n",
    "We have a CSV file containing samples of things a person might say and which intent it belongs to.\n",
    "\n",
    "You can read data files like the CSV file using pandas. Below you can find the code to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset summary\n",
    "Let's take a look at the dataset and see what it looks like.\n",
    "We're primarily interested in the amount of samples available and the class balance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's in the dataset\n",
    "The dataset contains sentences a user might say and the intent that it should be classified to.\n",
    "To get a sense of what's in the dataset, let's first print the `head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I need to hear the song Aspro Mavro from Bill ...</td>\n",
       "      <td>PlayMusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>play Yo Ho from the new york pops on Youtube</td>\n",
       "      <td>PlayMusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Play some seventies music by Janne Puurtinen o...</td>\n",
       "      <td>PlayMusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>play the Música Da Série De Filmes O Hobbit al...</td>\n",
       "      <td>PlayMusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Play Magic Sam from the thirties</td>\n",
       "      <td>PlayMusic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     intent\n",
       "0  I need to hear the song Aspro Mavro from Bill ...  PlayMusic\n",
       "1       play Yo Ho from the new york pops on Youtube  PlayMusic\n",
       "2  Play some seventies music by Janne Puurtinen o...  PlayMusic\n",
       "3  play the Música Da Série De Filmes O Hobbit al...  PlayMusic\n",
       "4                   Play Magic Sam from the thirties  PlayMusic"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class balance\n",
    "The value counts function shows you the number of records per unique value in a column.\n",
    "As you can see below, we have about the same number of samples for each intent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PlayMusic               2000\n",
       "GetWeather              2000\n",
       "BookRestaurant          1973\n",
       "SearchScreeningEvent    1959\n",
       "RateBook                1956\n",
       "SearchCreativeWork      1954\n",
       "Name: intent, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['intent'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of records\n",
    "Let's take a look at the total number of records in the dataset.\n",
    "This should be the sum of all the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11842 entries, 0 to 11841\n",
      "Data columns (total 2 columns):\n",
      "text      11842 non-null object\n",
      "intent    11842 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 185.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The info method shows the total number of entries and the number of non-null entries.\n",
    "They look the same, so we don't have any records without an intent label. \n",
    "\n",
    "Great, it looks like the data is usable for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Exploratory data analysis\n",
    "The next step is to perform uses exploratory data analysis techniques to get a sense of the data.\n",
    "Later on we will be building a model that classifies sequences. This has some specific requirements:\n",
    "\n",
    " * The length of the sequences\n",
    " * The distribution of the word counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining information about sequence length\n",
    "The problem with recurrent neural networks is that they are sensitive to the length of the sequences used to train the model.\n",
    "If there's a large difference in length, you will end up with a network that doesn't train well.\n",
    "\n",
    "Ideally the sentences should be equal in length or at least close enough.\n",
    "\n",
    "To check if this is the case we need to visualize the distribution of the sequence lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's tokenize the sentences first and calculate the length of each resulting sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [word_tokenize(s.lower()) for s in df['text'].tolist()]\n",
    "sequence_lengths = [len(s) for s in tokenized_sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization is easily done using matplotlib. The Y-axis displays the number of sequences of a given length. The X-axis displays the length of the sequences.\n",
    "\n",
    "Notice that we group together the sequence lengths in buckets. So some resolution is lost. \n",
    "But it does give a good idea of what we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAGvZJREFUeJzt3X20XXV95/H3x4Dg05RQgosmoaEa\n26Jto6RAa6eLokVEp2ArU5iOpizWSp2Bip1Ox2BnxkdmYEalY6tM4xAFa43UhzELGDFFqLWVh4Ax\nECglhVRiskhsgJbS0iZ854/zu3oI9+HseM899ybv11pnnb2/+7f3/p694H6z92/v/UtVIUnSoJ41\n6gQkSXOLhUOS1ImFQ5LUiYVDktSJhUOS1ImFQ5LUiYVDktSJhUOS1ImFQ5LUySGjTmAYjjrqqFqy\nZMmo05CkOeWOO+74dlUtmKrdAVk4lixZwoYNG0adhiTNKUn+epB2XqqSJHVi4ZAkdWLhkCR1YuGQ\nJHVi4ZAkdWLhkCR1YuGQJHVi4ZAkdWLhkCR1MvQnx5PMAzYA36qq1yc5DlgLHAncCbypqv4pyWHA\n1cAJwN8Av1xVW9s2LgbOB/YCb62qG4ad9ygsWXXdyPa99dLXjWzfkuaWmTjjuAi4t2/+MuDyqloK\nPEKvINC+H6mqFwOXt3YkOR44B3gpcDrwkVaMJEkjMNTCkWQR8Drg/7T5AKcCn2lNrgLOatNntnna\n8le19mcCa6vqyap6ENgCnDjMvCVJExv2GcfvAP8JeKrNfz/waFXtafPbgIVteiHwEEBb/lhr/534\nOOt8R5KVSTYk2bBr167p/h2SpGZohSPJ64GdVXVHf3icpjXFssnW+W6ganVVLa+q5QsWTPlWYEnS\nfhpm5/grgV9IcgZwOPAv6J2BHJHkkHZWsQjY3tpvAxYD25IcAnwfsLsvPqZ/HUnSDBvaGUdVXVxV\ni6pqCb3O7S9X1a8ANwFvbM1WAF9o0+vaPG35l6uqWvycJIe1O7KWArcNK29J0uRGMZDT24G1Sd4H\nfB24ssWvBD6RZAu9M41zAKpqc5JrgHuAPcAFVbV35tOWJMEMFY6quhm4uU0/wDh3RVXVPwJnT7D+\nJcAlw8tQkjQonxyXJHVi4ZAkdWLhkCR1YuGQJHVi4ZAkdWLhkCR1YuGQJHVi4ZAkdWLhkCR1YuGQ\nJHVi4ZAkdWLhkCR1Moq342oWWrLqupHsd+ulrxvJfiXtP884JEmdWDgkSZ1YOCRJnQytcCQ5PMlt\nSb6RZHOSd7f4x5M8mGRj+yxr8ST5UJItSTYleUXftlYkub99Vky0T0nS8A2zc/xJ4NSqejzJocBX\nk/y/tuy3quoz+7R/Lb3xxJcCJwFXACclORJ4J7AcKOCOJOuq6pEh5i5JmsDQzjiq5/E2e2j71CSr\nnAlc3da7BTgiyTHAa4D1VbW7FYv1wOnDyluSNLmh9nEkmZdkI7CT3h//W9uiS9rlqMuTHNZiC4GH\n+lbf1mITxSVJIzDUwlFVe6tqGbAIODHJy4CLgR8BfhI4Enh7a57xNjFJ/GmSrEyyIcmGXbt2TUv+\nkqRnmpG7qqrqUeBm4PSq2tEuRz0JfAw4sTXbBizuW20RsH2S+L77WF1Vy6tq+YIFC4bwKyRJMNy7\nqhYkOaJNPwd4NfAXrd+CJAHOAu5uq6wD3tzurjoZeKyqdgA3AKclmZ9kPnBai0mSRmCYd1UdA1yV\nZB69AnVNVV2b5MtJFtC7BLUReEtrfz1wBrAFeAI4D6Cqdid5L3B7a/eeqto9xLwlSZMYWuGoqk3A\ny8eJnzpB+wIumGDZGmDNtCYoSdovPjkuSerEwiFJ6sTCIUnqxMIhSerEwiFJ6sTCIUnqxMIhSerE\nwiFJ6sTCIUnqxMIhSerEwiFJ6sTCIUnqxMIhSerEwiFJ6sTCIUnqxMIhSerEwiFJ6mSYY44fnuS2\nJN9IsjnJu1v8uCS3Jrk/yaeTPLvFD2vzW9ryJX3burjF70vymmHlLEma2jDPOJ4ETq2qnwCWAacn\nORm4DLi8qpYCjwDnt/bnA49U1YuBy1s7khwPnAO8FDgd+Egbx1ySNAJDKxzV83ibPbR9CjgV+EyL\nXwWc1abPbPO05a9KkhZfW1VPVtWDwBbgxGHlLUma3FD7OJLMS7IR2AmsB/4KeLSq9rQm24CFbXoh\n8BBAW/4Y8P398XHW6d/XyiQbkmzYtWvXMH6OJIkhF46q2ltVy4BF9M4SfnS8Zu07EyybKL7vvlZX\n1fKqWr5gwYL9TVmSNIUZuauqqh4FbgZOBo5IckhbtAjY3qa3AYsB2vLvA3b3x8dZR5I0w4Z5V9WC\nJEe06ecArwbuBW4C3tiarQC+0KbXtXna8i9XVbX4Oe2uq+OApcBtw8pbkjS5Q6Zust+OAa5qd0A9\nC7imqq5Ncg+wNsn7gK8DV7b2VwKfSLKF3pnGOQBVtTnJNcA9wB7ggqraO8S8JUmTGFrhqKpNwMvH\niT/AOHdFVdU/AmdPsK1LgEumO0dJUnc+OS5J6sTCIUnqxMIhSerEwiFJ6sTCIUnqxMIhSerEwiFJ\n6sTCIUnqxMIhSerEwiFJ6mSgwpHkRUkOa9OnJHnr2AsMJUkHl0HPOD4L7E3yYnovIzwO+MOhZSVJ\nmrUGLRxPtVH53gD8TlX9Br2330qSDjKDFo5/TnIuvfEyrm2xQ4eTkiRpNhu0cJwH/BRwSVU92AZU\n+oPhpSVJmq0GGo+jqu5J8nbg2Db/IHDpMBOTJM1Og95V9a+AjcAX2/yyJOumWGdxkpuS3Jtkc5KL\nWvxdSb6VZGP7nNG3zsVJtiS5L8lr+uKnt9iWJKv254dKkqbHoCMAvoveqH03A1TVxna5ajJ7gN+s\nqjuTvAC4I8n6tuzyqnp/f+Mkx9MbLvalwA8Af5zkJW3xh4GfB7YBtydZV1X3DJi7JGkaDVo49lTV\nY0n6YzXZClW1A9jRpv8uyb3AwklWORNYW1VPAg+2scfHhpjd0oacJcna1tbCIUkjMGjn+N1J/g0w\nL8nSJL8L/PmgO0myhN7447e20IVJNiVZk2R+iy0EHupbbVuLTRSXJI3AoIXj1+ldQnqS3oN/jwFv\nG2TFJM+n9wDh26rqb4ErgBcBy+idkXxgrOk4q9ck8X33szLJhiQbdu3aNUhqkqT9MOhdVU8Av90+\nA0tyKL2i8cmq+lzb1sN9yz/Kd58L2QYs7lt9EbC9TU8U789xNbAaYPny5ZNeRpMk7b9B76pa3/9u\nqiTzk9wwxTqh93qSe6vqg33x/ifO3wDc3abXAeckOax1vC8FbgNuB5YmOS7Js+l1oE96R5ckaXgG\n7Rw/qqoeHZupqkeSHD3FOq8E3gTclWRji70DODfJMnqXm7YCv9a2uTnJNfQ6vfcAF1TVXoAkFwI3\nAPOANVW1ecC8JUnTbNDC8VSSY6vqmwBJfpCp76r6KuP3T1w/yTqXAJeME79+svUkSTNn0MLx28BX\nk/xJm/9ZYOVwUpIkzWaDdo5/MckrgJPpnUX8RlV9e6iZSZJmpUHPOAAOA3a3dY5PQlV9ZThpSZJm\nq4EKR5LLgF8GNgNPtXABFg5JOsgMesZxFvDD7XUgkqSD2KBPjj+AAzdJkhj8jOMJYGOSG+m9dgSA\nqnrrULKSJM1agxaOdfi0tiSJwW/HvSrJc4Bjq+q+IeckSZrFhjYCoCTpwDRo5/i76A2q9Cj0RgAE\nphoBUJJ0ABq0cOypqsf2ifnqckk6CA3aOf60EQCBt9JhBEBJ0oFj6CMASpIOLFOecSSZB7y7qn6L\njiMASpIOPFMWjqram+SEmUhGB58lq64byX63Xvq6kexXOhAMeqnq60nWJXlTkl8c+0y2QpLFSW5K\ncm+SzUkuavEj21C097fv+S2eJB9KsiXJpvYa97FtrWjt70+yYr9/rSTpezZo5/iRwN8Ap/bFCvjc\nJOvsAX6zqu5M8gLgjiTrgV8FbqyqS5OsAlYBbwdeS2+c8aXAScAVwElJjgTeCSxv+7wjybqqemTA\n3CVJ02jQPo5NVXV5lw1X1Q5gR5v+uyT3AguBM4FTWrOrgJvpFY4zgaurqoBbkhyR5JjWdn1V7W75\nrAdOBz7VJR9J0vSY8lJVVe0FfuF72UmSJcDLgVuBF7aiMlZcjm7NFgIP9a22rcUmikuSRmDQS1V/\nnuT3gE8Dfz8WrKo7p1oxyfOBzwJvq6q/TTJh03FiNUl83/2spI2Dfuyxx06VliRpPw1aOH66fb+n\nL1Y8vc/jGZIcSq9ofLKqxvpDHk5yTFXtaJeidrb4NmBx3+qLgO0tfso+8Zv33VdVrQZWAyxfvtyn\n2iVpSAZ9O+7Pdd1weqcWVwL3VtUH+xatA1YAl7bvL/TFL0yyll7n+GOtuNwA/Lexu6+A04CLu+Yj\nSZoeg445/l/Hi1fVe8aLN68E3gTclWRji72DXsG4Jsn5wDeBs9uy64EzgC30Bo46r+1jd5L3Are3\ndu8Z6yiXJM28QS9V/X3f9OHA64F7J1uhqr7K+P0TAK8ap30BF0ywrTXAmoEylSQN1aCXqj7QP5/k\n/TgioCQdlAZ9cnxfzwV+aDoTkSTNDYP2cdzFd2+BnQcs4Ol3WEmSDhKD9nG8vm96D/BwVe0ZQj6S\npFlu0EtVxwC7q+qvq+pbwOFJThpiXpKkWWrQwnEF8Hjf/BMtJkk6yAxaONJulwWgqp5i8MtckqQD\nyKCF44Ekb01yaPtcBDwwzMQkSbPToIXjLfTeV/Uteu+OOon2QkFJ0sFl0AcAdwLnDDkXSdIcMNAZ\nR5KrkhzRNz8/ia8AkaSD0KCXqn68qh4dm2nDtr58OClJkmazQQvHs/pea04bB9y7qiTpIDToH/8P\nAF9L8kdt/mzgkuGkJEmazQbtHL86yRZgOfAUcF5VfW2omUmSZqVBO8cvAn4f+H7gaOD3k/z6MBOT\nJM1Og16qOh84uar+HiDJZcDXgN8dVmKSpNlp4FeOAHv75vcy8eh+vRWSNUl2Jrm7L/auJN9KsrF9\nzuhbdnGSLUnuS/KavvjpLbYlyaoB85UkDcmgZxwfA25N8vk2fxZw5RTrfBz4PeDqfeKXV9X7+wNJ\njqf3gOFLgR8A/jjJS9riDwM/T++J9duTrKuqewbMW5I0zQbtHP9gkpuBn6F3pnFeVX19inW+kmTJ\ngHmcCaytqieBB1tH/Ilt2ZaqegAgydrW1sIhSSMy8LMYVXUncOc07PPCJG8GNgC/2R4mXAjc0tdm\nW4sBPLRPfNxxQJKspL0/69hjj52GNCVJ49nfMcf31xXAi4BlwA56z4fA+P0lNUn8mcGq1VW1vKqW\nL1iwYDpylSSNY0af/q6qh8emk3wUuLbNbgMW9zVdBGxv0xPFJUkjMKNnHEmO6Zt9AzB2x9U64Jwk\nhyU5DlgK3AbcDixNclySZ9PrQF83kzlLkp5uaGccST4FnAIclWQb8E7glCTL6F1u2gr8GkBVbU5y\nDb1O7z3ABVW1t23nQuAGYB6wpqo2DytnSdLUhlY4qurcccIT3sJbVZcwzvuvqup64PppTE2S9D2Y\n6c5xSdIcZ+GQJHVi4ZAkdeJgTONYsuq6UacgSbOWZxySpE4sHJKkTiwckqROLBySpE4sHJKkTiwc\nkqROLBySpE4sHJKkTiwckqROLBySpE4sHJKkTiwckqROhlY4kqxJsjPJ3X2xI5OsT3J/+57f4kny\noSRbkmxK8oq+dVa09vcnWTGsfCVJgxnmGcfHgdP3ia0CbqyqpcCNbR7gtfTGGV8KrASugF6hoTfk\n7EnAicA7x4qNJGk0hlY4quorwO59wmcCV7Xpq4Cz+uJXV88twBFJjgFeA6yvqt1V9QiwnmcWI0nS\nDJrpPo4XVtUOgPZ9dIsvBB7qa7etxSaKS5JGZLZ0jmecWE0Sf+YGkpVJNiTZsGvXrmlNTpL0XTNd\nOB5ul6Bo3ztbfBuwuK/dImD7JPFnqKrVVbW8qpYvWLBg2hOXJPXMdOFYB4zdGbUC+EJf/M3t7qqT\ngcfapawbgNOSzG+d4qe1mCRpRIY25niSTwGnAEcl2Ubv7qhLgWuSnA98Ezi7Nb8eOAPYAjwBnAdQ\nVbuTvBe4vbV7T1Xt2+EuSZpBQyscVXXuBIteNU7bAi6YYDtrgDXTmJok6XswWzrHJUlzhIVDktSJ\nhUOS1ImFQ5LUiYVDktSJhUOS1ImFQ5LUiYVDktSJhUOS1ImFQ5LUiYVDktSJhUOS1ImFQ5LUiYVD\nktSJhUOS1ImFQ5LUiYVDktTJSApHkq1J7kqyMcmGFjsyyfok97fv+S2eJB9KsiXJpiSvGEXOkqSe\nUZ5x/FxVLauq5W1+FXBjVS0FbmzzAK8FlrbPSuCKGc9UkvQdQxtzfD+cCZzSpq8Cbgbe3uJXt3HJ\nb0lyRJJjqmrHSLLUAWHJqutGtu+tl75uZPuWpsOozjgK+FKSO5KsbLEXjhWD9n10iy8EHupbd1uL\nPU2SlUk2JNmwa9euIaYuSQe3UZ1xvLKqtic5Glif5C8maZtxYvWMQNVqYDXA8uXLn7FckjQ9RnLG\nUVXb2/dO4PPAicDDSY4BaN87W/NtwOK+1RcB22cuW0lSvxkvHEmel+QFY9PAacDdwDpgRWu2AvhC\nm14HvLndXXUy8Jj9G5I0OqO4VPVC4PNJxvb/h1X1xSS3A9ckOR/4JnB2a389cAawBXgCOG/mU5Yk\njZnxwlFVDwA/MU78b4BXjRMv4IIZSE2SNACfHJckdWLhkCR1YuGQJHVi4ZAkdWLhkCR1YuGQJHVi\n4ZAkdWLhkCR1YuGQJHVi4ZAkdWLhkCR1YuGQJHVi4ZAkdWLhkCR1MqqhY6WD1pJV141kv1svfd1I\n9qsDj2cckqRO5kzhSHJ6kvuSbEmyatT5SNLBak4UjiTzgA8DrwWOB85Ncvxos5Kkg9Nc6eM4EdjS\nhp0lyVrgTOCekWYlzSGj6lsB+1cONHOlcCwEHuqb3wacNKJcJHXkDQEHlrlSODJOrJ7WIFkJrGyz\njye5b+hZTY+jgG+POon9MFfzhrmbu3l3lMu+p9UPxuP9g4M0miuFYxuwuG9+EbC9v0FVrQZWz2RS\n0yHJhqpaPuo8upqrecPczd28Z5Z5T2xOdI4DtwNLkxyX5NnAOcC6EeckSQelOXHGUVV7klwI3ADM\nA9ZU1eYRpyVJB6U5UTgAqup64PpR5zEEc+7yWjNX84a5m7t5zyzznkCqaupWkiQ1c6WPQ5I0S1g4\nRijJ1iR3JdmYZMOo85lIkjVJdia5uy92ZJL1Se5v3/NHmeN4Jsj7XUm+1Y75xiRnjDLH8SRZnOSm\nJPcm2Zzkohaf1cd8krxn9TFPcniS25J8o+X97hY/Lsmt7Xh/ut2YM2tMkvfHkzzYd7yXTfu+vVQ1\nOkm2AsuralbfK57kZ4HHgaur6mUt9j+A3VV1aXt32Pyqevso89zXBHm/C3i8qt4/ytwmk+QY4Jiq\nujPJC4A7gLOAX2UWH/NJ8v7XzOJjniTA86rq8SSHAl8FLgL+A/C5qlqb5H8D36iqK0aZa79J8n4L\ncG1VfWZY+/aMQ1Oqqq8Au/cJnwlc1aavovcHYlaZIO9Zr6p2VNWdbfrvgHvpvT1hVh/zSfKe1arn\n8TZ7aPsUcCow9sd3Nh7vifIeOgvHaBXwpSR3tCff55IXVtUO6P3BAI4ecT5dXJhkU7uUNasu9+wr\nyRLg5cCtzKFjvk/eMMuPeZJ5STYCO4H1wF8Bj1bVntZkG7OwCO6bd1WNHe9L2vG+PMlh071fC8do\nvbKqXkHvrb8XtEsrGq4rgBcBy4AdwAdGm87Ekjwf+Czwtqr621HnM6hx8p71x7yq9lbVMnpvpTgR\n+NHxms1sVlPbN+8kLwMuBn4E+EngSGDaL2daOEaoqra3753A5+n9BztXPNyuaY9d29454nwGUlUP\nt//ZngI+yiw95u2a9WeBT1bV51p41h/z8fKeK8ccoKoeBW4GTgaOSDL2rNszXnM0m/TlfXq7ZFhV\n9STwMYZwvC0cI5Lkea0DkSTPA04D7p58rVllHbCiTa8AvjDCXAY29oe3eQOz8Ji3Ts8rgXur6oN9\ni2b1MZ8o79l+zJMsSHJEm34O8Gp6/TM3AW9szWbj8R4v77/o+8dF6PXLTPvx9q6qEUnyQ/TOMqD3\nBP8fVtUlI0xpQkk+BZxC762bDwPvBP4vcA1wLPBN4OyqmlUd0RPkfQq9SyYFbAV+bazfYLZI8jPA\nnwJ3AU+18Dvo9RfM2mM+Sd7nMouPeZIfp9f5PY/eP6avqar3tP9H19K73PN14N+2f8XPCpPk/WVg\nAb23im8E3tLXiT49+7ZwSJK68FKVJKkTC4ckqRMLhySpEwuHJKkTC4ckqRMLhzRLtbecvnHqlp23\n+46+6SX9bw+WBmHhkA4+75i6iTQxC4cOGO1p/Ova+AR3J/nlFj8hyZ+0l0ne0Pdk7Qmt7deS/M+x\nf3kn+dUkv9e33WuTnNKmT2vt70zyR+29TGNjq7y7xe9K8iMt/vwkH2uxTUl+abLtTPLbJvoNNye5\nLL1xGf4yyb9s8ecmuabt89PpjSuxPMmlwHPSG6fhk23z85J8NL0xHb7UnkKWJmTh0IHkdGB7Vf1E\nG3/ji+3dSb8LvLGqTgDWAGNP6H8MeGtV/dQgG09yFPCfgVe3l1NuoDdmw5hvt/gVwH9ssf8CPFZV\nP1ZVPw58eYDt7LvfyX4DwCFVdSLwNnpPxwP8e+CRts/3AicAVNUq4B+qallV/UpruxT4cFW9FHgU\n+KVBjocOXodM3USaM+4C3p/kMnoD2fxpe1voy4D1vVf3MA/YkeT7gCOq6k/aup+g95biyZwMHA/8\nWdvWs4Gv9S0fexnhHcAvtulXA+eMNaiqR5K8fort7OuHx/sNE+x3SZv+GeB/tX3enWTTJNt/sKo2\njrMNaVwWDh0wquovk5wAnAH89yRfovc+sM37nlW0l8NN9L6dPTz9bPzwsdXojXlw7gTrjb3HaC/f\n/X8r4+xnqu3sK4zzGwbY76D637+0F/BSlSblpSodMJL8APBEVf0B8H7gFcB9wIIkP9XaHJrkpe01\n1I+1F/MB/ErfprYCy5I8K8livvta6luAVyZ5cdvWc5O8ZIq0vgRc2Jfj/P3Yzri/YYr9fpXekK0k\nOR74sb5l/9wuf0n7xcKhA8mPAbelNyLabwPvq6p/ovdq7MuSfIPe20J/urU/D/hwkq8B/9C3nT8D\nHqRd+gLGhkPdRW/c70+1Sz+30BswZzLvA+a3zvpvAD/XdTtT/IaJfIResdlEbyCfTcBjbdlqYFNf\n57jUiW/HlfjOUKfXtk71OS/JPODQqvrHJC8CbgRe0oqQ9D2xj0M6MD0XuKldkgrw7ywami6ecUiS\nOrGPQ5LUiYVDktSJhUOS1ImFQ5LUiYVDktSJhUOS1Mn/B4nxljITcqPUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1aee3820a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sequence_lengths)\n",
    "\n",
    "plt.xlabel('sequence length')\n",
    "plt.ylabel('occurrences')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining information about the word distribution\n",
    "The second problem that we have with our neural network is that the number of different words also influences the performance of the network. Typically you don't want any words that are generic in nature. But since we're classifying sequences you need enough information to maintain the relationship between different words in the sentence that are identifying for an intent.\n",
    "\n",
    "The code below builds a list of word counts from all the tokenized sentences in the dataset.\n",
    "This involves two loops, the first one iterates over all sentences, the second loop iterates over \n",
    "the words in each sentence. \n",
    "\n",
    "We build a dictionary that we than convert into a new dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = {\n",
    "    \n",
    "}\n",
    "\n",
    "for s in tokenized_sentences:\n",
    "    for w in s:\n",
    "        if not w in word_counts:\n",
    "            word_counts[w] = 1\n",
    "        else:\n",
    "            word_counts[w] = word_counts[w] + 1\n",
    "            \n",
    "df_wordcount = pd.DataFrame.from_dict(word_counts, orient='index')\n",
    "df_wordcount.columns = ['count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of the data, we are interested in frequently occurring terms first. This information is most easily obtained by sorting the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>6723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>4215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>3275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>for</th>\n",
       "      <td>3008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>2451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>2101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>at</th>\n",
       "      <td>2097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>play</th>\n",
       "      <td>1904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>1725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>book</th>\n",
       "      <td>1607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>1514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what</th>\n",
       "      <td>1486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>find</th>\n",
       "      <td>1400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>me</th>\n",
       "      <td>1231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>?</th>\n",
       "      <td>1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>1111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>1054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on</th>\n",
       "      <td>1041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count\n",
       "the     6723\n",
       "a       4215\n",
       "in      3275\n",
       "for     3008\n",
       "of      2451\n",
       "i       2101\n",
       "at      2097\n",
       ".       2026\n",
       "play    1904\n",
       "to      1725\n",
       "book    1607\n",
       "is      1514\n",
       "what    1486\n",
       "find    1400\n",
       "me      1231\n",
       "?       1162\n",
       "6       1130\n",
       "movie   1111\n",
       "from    1054\n",
       "on      1041"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_wordcount.sort_values(by=['count'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we need to remove stopwords from the dataset. For now, we're done exploring the data.\n",
    "Let's build a model based on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Build a model\n",
    "To detect the intents we're going to build a recurrent model. We're going to build our model using Microsoft Cognitive Toolkit an open source deep learning framework from Microsoft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "from cntk.layers import Embedding, Sequential, LSTM, Dense, Recurrence\n",
    "from cntk import Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input size of the neural network is equal to the number of unique words in the dataset.\n",
    "The output size of the neural network should be equal to the number of intents we understand.\n",
    "\n",
    "We record these values below, so that they are easily accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_size = len(df_wordcount.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_intents = len(df['intent'].astype('category').unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is made up out of an embedding layer, a recurrent layer of one LSTM unit and a dense layer for classification.\n",
    "This is expressed as a function call to different layer types. Notice that we include a `C.sequence.last` layer. This ensures that the input of the `Dense` layer is the output of the last iteration for the `LSTM` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Embedding(128), Recurrence(LSTM(128,64)),C.sequence.last, Dense(num_intents)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the network we need to provide two input variables. The features variable a sequence variable where the dimension of each element is equal to the number of unique words in the dataset. The labels variable is a regular variable which dimension is equal to the number of intents we can detect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = C.sequence.input_variable(input_size)\n",
    "labels = C.input_variable(num_intents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the model we need to convert it into a function. The model we created is a graph without an input attached to it. \n",
    "Of course if you want to train a model, you need to attach an input to it. This is done by invoking the model with the input placeholder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Encode data\n",
    "The neural network we've created only understands vectors of numbers. But we don't have any numbers in our data.\n",
    "The fix is to encode the data into numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding sentences into sequences of features\n",
    "First we encode each sentence into a sequence of one-hot vectors.\n",
    "For this we need to make a lookup table that maps a word to an index in the vector.\n",
    "Then we encode all the values using this lookup table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words = [w for w,c in word_counts.items()]\n",
    "word2idx = dict(zip(unique_words, range(0,len(unique_words))))\n",
    "idx2word = dict(zip(range(0, len(unique_words)), unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(index, size):\n",
    "    v = np.zeros(size)\n",
    "    v[index] = 1.0\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_sequence(sequence):\n",
    "    encoded_list = [one_hot(word2idx[w], len(unique_words)) for w in sequence]\n",
    "    return np.asarray(encoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data = [encode_sequence(s) for s in tokenized_sentences]\n",
    "feature_data = np.asarray(feature_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding labels into vectors\n",
    "The labels can be encoded into a one-hot vectors as well. But with a much simpler trick.\n",
    "You can ask pandas to create dummy columns. What happens is the following.\n",
    "\n",
    " * A new column is created for every unique value in the columns that you want dummies for\n",
    " * All records are converted, by assigning a value to the dummy column that matches the value in the dummified column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.get_dummies(df[['intent']])\n",
    "idx2label = df_labels.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = df_labels.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Train the model\n",
    "Once you have a model setup, you can start to train it.\n",
    "First we need to configure the model as a mathmatical function that we can optmize.\n",
    "Next we need to make sure that we can feed input into the model in the right shape. This means we need to encode the sequences and labels into correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = model(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to train the model to correctly classify an input sequence. For this you need to define a loss function. The loss function calculates the distance between the actual classification and the one we expected to get as output. \n",
    "\n",
    "Ultimately, we want the output of this function to be as close as possible to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = C.cross_entropy_with_softmax(z, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A loss value that is close to zero, doesn't mean our model actually works. To measure the performance we need to define a metric. The metric we're going to use is the classification error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metric = C.classification_error(z, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained using a trainer object. This object needs to have the model function, a loss, a metric and an optmizer it should use for optimizing the parameters of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(z, (loss,metric), C.sgd(z.parameters, 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run all the values through the model to train it. We do this in mini batches to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-dict argument (list) is not supported for nodes with more than one input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-179-89a3845fd956>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mlabels_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mminibatch_offset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mminibatch_offset\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_minibatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\cntk\\train\\trainer.py\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[1;34m(self, arguments, outputs, device, is_sweep_end)\u001b[0m\n\u001b[0;32m    147\u001b[0m                 \u001b[0mall_args\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             arguments = sanitize_var_map(tuple(all_args), arguments,\n\u001b[1;32m--> 149\u001b[1;33m                 extract_values_from_minibatch_data = False, device=device)\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mcontains_minibatch_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\cntk\\internal\\sanitize.py\u001b[0m in \u001b[0;36msanitize_var_map\u001b[1;34m(op_arguments, arguments, precision, device, extract_values_from_minibatch_data)\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m             raise ValueError(\n\u001b[1;32m--> 385\u001b[1;33m                 'non-dict argument (%s) is not supported for nodes with more than one input' % type(arguments).__name__)\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprecision\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: non-dict argument (list) is not supported for nodes with more than one input"
     ]
    }
   ],
   "source": [
    "for minibatch_offset in range(0, len(feature_data), 32):\n",
    "    features_batch = feature_data[minibatch_offset:minibatch_offset+32]\n",
    "    labels_batch = label_data[minibatch_offset:minibatch_offset+32]\n",
    "    \n",
    "    trainer.train_minibatch(features_batch,labels_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
